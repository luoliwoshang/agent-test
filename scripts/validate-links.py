#!/usr/bin/env python3
"""
æ–‡æ¡£é“¾æ¥å®‰å…¨éªŒè¯è„šæœ¬
æ£€æŸ¥æ–‡æ¡£ä¸­çš„é“¾æ¥æ˜¯å¦å®‰å…¨ï¼Œé˜²æ­¢æ¶æ„é“¾æ¥å…¥åº“
"""

import re
import sys
import os
import glob
from urllib.parse import urlparse
from pathlib import Path

# å±é™©åŸŸåé»‘åå•
DANGEROUS_DOMAINS = {
    # æ¶æ„è½¯ä»¶/é’“é±¼ç½‘ç«™å¸¸ç”¨åŸŸå
    'bit.ly', 'tinyurl.com', 'goo.gl', 't.co',  # çŸ­é“¾æ¥æœåŠ¡
    'malware.com', 'phishing.com', 'virus.com',  # æ˜æ˜¾æ¶æ„åŸŸå
    # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤š
}

# å¯ç–‘åè®®
DANGEROUS_PROTOCOLS = {
    'ftp', 'file', 'javascript', 'data', 'vbscript'
}

# å…è®¸çš„å®‰å…¨åŸŸå
SAFE_DOMAINS = {
    'github.com', 'githubusercontent.com',
    'docs.github.com', 'help.github.com',
    'stackoverflow.com', 'developer.mozilla.org',
    'w3.org', 'ietf.org', 'rfc-editor.org',
    'nodejs.org', 'python.org', 'golang.org',
    'microsoft.com', 'google.com', 'anthropic.com',
    'ngrok.com', 'render.com', 'localhost',
    'serveo.net', 'localtunnel.me',
    'example.com', 'example.org',  # ç¤ºä¾‹åŸŸå
}

# æœ¬åœ°é“¾æ¥æ¨¡å¼
LOCAL_PATTERNS = [
    r'^#',           # é”šç‚¹é“¾æ¥
    r'^\./',         # ç›¸å¯¹è·¯å¾„
    r'^\.\.',        # ä¸Šçº§ç›®å½•
    r'^/',           # ç»å¯¹è·¯å¾„
    r'^[^/]+\.md$',  # åŒç›®å½•markdownæ–‡ä»¶
]

def extract_links_from_markdown(content):
    """ä»markdownå†…å®¹ä¸­æå–æ‰€æœ‰é“¾æ¥"""
    links = []
    
    # Markdowné“¾æ¥æ ¼å¼: [text](url)
    markdown_links = re.findall(r'\[([^\]]*)\]\(([^)]+)\)', content)
    for text, url in markdown_links:
        links.append({'text': text, 'url': url, 'type': 'markdown'})
    
    # HTMLé“¾æ¥æ ¼å¼: <a href="url">text</a>
    html_links = re.findall(r'<a[^>]+href=["\']([^"\']+)["\'][^>]*>([^<]*)</a>', content, re.IGNORECASE)
    for url, text in html_links:
        links.append({'text': text, 'url': url, 'type': 'html'})
    
    # çº¯URL (http/httpså¼€å¤´çš„ç‹¬ç«‹URL)
    url_pattern = r'https?://[^\s<>"\'\[\]()]+[^\s<>"\'\[\]().,;:]'
    pure_urls = re.findall(url_pattern, content)
    for url in pure_urls:
        # é¿å…é‡å¤ï¼ˆå·²ç»åœ¨markdownæˆ–htmlé“¾æ¥ä¸­çš„URLï¼‰
        if not any(link['url'] == url for link in links):
            links.append({'text': url, 'url': url, 'type': 'plain'})
    
    return links

def is_local_link(url):
    """æ£€æŸ¥æ˜¯å¦ä¸ºæœ¬åœ°é“¾æ¥"""
    for pattern in LOCAL_PATTERNS:
        if re.match(pattern, url):
            return True
    return False

def validate_url(url):
    """éªŒè¯å•ä¸ªURLçš„å®‰å…¨æ€§"""
    issues = []
    
    # è·³è¿‡æœ¬åœ°é“¾æ¥
    if is_local_link(url):
        return issues
    
    try:
        parsed = urlparse(url)
        
        # æ£€æŸ¥åè®®
        if parsed.scheme.lower() in DANGEROUS_PROTOCOLS:
            issues.append(f"å±é™©åè®®: {parsed.scheme}")
        
        # æ£€æŸ¥åŸŸå
        domain = parsed.netloc.lower()
        if domain:
            # ç§»é™¤ç«¯å£å·
            domain = domain.split(':')[0]
            
            # æ£€æŸ¥æ˜¯å¦åœ¨é»‘åå•ä¸­
            if domain in DANGEROUS_DOMAINS:
                issues.append(f"é»‘åå•åŸŸå: {domain}")
            
            # æ£€æŸ¥å¯ç–‘æ¨¡å¼
            if re.search(r'\d+\.\d+\.\d+\.\d+', domain):  # IPåœ°å€
                if not domain.startswith('127.') and not domain.startswith('192.168.'):
                    issues.append(f"ä½¿ç”¨IPåœ°å€è€ŒéåŸŸå: {domain}")
            
            # æ£€æŸ¥è¿‡é•¿åŸŸå
            if len(domain) > 100:
                issues.append("åŸŸåè¿‡é•¿ï¼Œå¯èƒ½æ˜¯æ¶æ„åŸŸå")
            
            # æ£€æŸ¥éšæœºå­—ç¬¦åŸŸå
            if re.match(r'^[a-z0-9]{10,}\.', domain):
                issues.append("ç–‘ä¼¼éšæœºç”Ÿæˆçš„åŸŸå")
    
    except Exception as e:
        issues.append(f"URLè§£æé”™è¯¯: {e}")
    
    return issues

def validate_file(file_path):
    """éªŒè¯å•ä¸ªæ–‡ä»¶ä¸­çš„é“¾æ¥"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        return [{'file': file_path, 'error': f"æ–‡ä»¶è¯»å–é”™è¯¯: {e}"}]
    
    links = extract_links_from_markdown(content)
    issues = []
    
    for link in links:
        url = link['url']
        link_issues = validate_url(url)
        
        if link_issues:
            issues.append({
                'file': file_path,
                'text': link['text'],
                'url': url,
                'type': link['type'],
                'issues': link_issues
            })
    
    return issues

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å¼€å§‹éªŒè¯æ–‡æ¡£é“¾æ¥å®‰å…¨æ€§...")
    
    # æŸ¥æ‰¾æ‰€æœ‰markdownæ–‡ä»¶
    md_files = []
    for pattern in ['**/*.md', '*.md']:
        md_files.extend(glob.glob(pattern, recursive=True))
    
    if not md_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•markdownæ–‡ä»¶")
        return 1
    
    print(f"ğŸ“ æ‰¾åˆ° {len(md_files)} ä¸ªmarkdownæ–‡ä»¶")
    
    all_issues = []
    
    for file_path in md_files:
        print(f"  æ£€æŸ¥: {file_path}")
        issues = validate_file(file_path)
        all_issues.extend(issues)
    
    # è¾“å‡ºç»“æœ
    if not all_issues:
        print("âœ… æ‰€æœ‰é“¾æ¥éƒ½æ˜¯å®‰å…¨çš„ï¼")
        return 0
    
    print(f"\nâš ï¸  å‘ç° {len(all_issues)} ä¸ªæ½œåœ¨é—®é¢˜:")
    
    current_file = None
    for issue in all_issues:
        if 'error' in issue:
            print(f"\nâŒ {issue['file']}: {issue['error']}")
            continue
            
        if issue['file'] != current_file:
            current_file = issue['file']
            print(f"\nğŸ“„ {current_file}:")
        
        print(f"  ğŸ”— [{issue['text']}]({issue['url']})")
        print(f"     ç±»å‹: {issue['type']}")
        for problem in issue['issues']:
            print(f"     âš ï¸  {problem}")
    
    print(f"\nâŒ éªŒè¯å¤±è´¥: å‘ç° {len(all_issues)} ä¸ªå®‰å…¨é—®é¢˜")
    return 1

if __name__ == '__main__':
    sys.exit(main())